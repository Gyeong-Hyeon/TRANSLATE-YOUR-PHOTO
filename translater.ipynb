{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "translater.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "71KN3oobxAZU",
        "bsow-Z6D_20a"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gyeong-Hyeon/TRANSLATE-YOUR-PHOTO/blob/main/translater.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kcH4Ao50JWK"
      },
      "source": [
        "* https://manuscriptlink-society-file.s3-ap-northeast-1.amazonaws.com/kips/conference/2020fall/presentation/KIPS_C2020B0187.pdf\n",
        "\n",
        "* https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
        "\n",
        "* https://github.com/Kyubyong/cjk_trans\n",
        "\n",
        "번역 모델 선정\n",
        "* https://tech.kakaoenterprise.com/48\n",
        "\n",
        "Tokenizer:\n",
        "\n",
        "* English: https://github.com/alvations/sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24hJv0RYRCGB",
        "outputId": "e6e0af10-630c-4448-c7dd-5cd810fc9994"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71KN3oobxAZU"
      },
      "source": [
        "##Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzdHKWkyxZT7"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "wr = pd.read_excel('/content/drive/MyDrive/OCR/dataset/koen/4_문어체_한국문화_200226.xlsx')\n",
        "sp = pd.read_excel('/content/drive/MyDrive/OCR/dataset/koen/1_구어체(1)_200226.xlsx')\n",
        "sp2 = pd.read_excel('/content/drive/MyDrive/OCR/dataset/koen/1_구어체(2)_200226.xlsx')\n",
        "n = pd.read_excel('/content/drive/MyDrive/OCR/dataset/koen/3_문어체_뉴스(1)_200226.xlsx')\n",
        "n2 = pd.read_excel('/content/drive/MyDrive/OCR/dataset/koen/3_문어체_뉴스(2)_200226.xlsx')\n",
        "n3 = pd.read_excel('/content/drive/MyDrive/OCR/dataset/koen/3_문어체_뉴스(3)_200226.xlsx')\n",
        "n4 = pd.read_excel('/content/drive/MyDrive/OCR/dataset/koen/3_문어체_뉴스(4)_200226.xlsx')\n",
        "\n",
        "wr.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyPc-lDh9qMH"
      },
      "source": [
        "sp.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVCtmoZu9s85"
      },
      "source": [
        "n.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICnqre6B9422"
      },
      "source": [
        "wr.drop(columns='키워드', inplace=True)\n",
        "sp = sp.rename({'SID':'ID'}, axis='columns')\n",
        "sp2 = sp2.rename({'SID':'ID'}, axis='columns')\n",
        "n.drop(columns=['날짜','자동분류1','자동분류2','자동분류3','URL','언론사'], inplace=True)\n",
        "n2.drop(columns=['날짜','자동분류1','자동분류2','자동분류3','URL','언론사'], inplace=True)\n",
        "n3.drop(columns=['날짜','자동분류1','자동분류2','자동분류3','URL','언론사'], inplace=True)\n",
        "n4.drop(columns=['날짜','자동분류1','자동분류2','자동분류3','URL','언론사'], inplace=True)\n",
        "\n",
        "df = pd.concat([wr,sp,sp2,n,n2,n3,n4])\n",
        "print(len(df),len(wr)+len(sp)+len(sp2)+len(n)+len(n2)+len(n3)+len(n4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw2zSKFe2Gna"
      },
      "source": [
        "df.reset_index(drop=True, inplace=True)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBtbAgvgTlRS"
      },
      "source": [
        "df['번역문'].to_csv('en.txt',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsow-Z6D_20a"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVeYxEZvyiIH"
      },
      "source": [
        "!pip install -U sacremoses\n",
        "!set -x \\\n",
        "&& pip install konlpy \\\n",
        "&& curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh | bash -x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyTtja2JwrWO"
      },
      "source": [
        "%cd /content/drive/MyDrive/OCR/dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TOBwW5KfeV6"
      },
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab().morphs\n",
        "sentence=df[k][0]\n",
        "morph_sentence= []\n",
        "count = 0\n",
        "for token_mecab in mecab(sentence):\n",
        "    token_mecab_save = token_mecab\n",
        "    if count > 0:\n",
        "        token_mecab_save = \"##\" + token_mecab_save  # 앞에 ##를 부친다\n",
        "        morph_sentence.append(token_mecab_save)\n",
        "    else:\n",
        "        morph_sentence.append(token_mecab_save)\n",
        "        count += 1\n",
        "# 문장단위 저장\n",
        "print('mecab check :', ' '.join(morph_sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9QLazcg_HaZ"
      },
      "source": [
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "from konlpy.tag import Mecab\n",
        "from sacremoses import MosesTokenizer\n",
        "\n",
        "class preprocessing:\n",
        "    \n",
        "    def __init__(self):\n",
        "        punct = '\"“”#$%&\\'()*+,-/:;<=>@[\\\\]^_`{|}~'\n",
        "        self.table = str.maketrans({key: None for key in punct})\n",
        "    \n",
        "    @staticmethod\n",
        "    def unicodeToAscii(s):\n",
        "        print('unicode')\n",
        "        s= ''.join(\n",
        "            c for c in unicodedata.normalize('NFD', s)\n",
        "            if unicodedata.category(c) != 'Mn'\n",
        "        )\n",
        "        print(s)\n",
        "        return s\n",
        "\n",
        "    # Lowercase, trim, and remove non-letter characters\n",
        "    @staticmethod\n",
        "    def normalizeString(s):\n",
        "        s = preprocessing.unicodeToAscii(s.lower().strip())\n",
        "        s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "        s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "        print('normal',s)\n",
        "        return s\n",
        "    \n",
        "    def extract_stem(self, df, lang= 'kr', generation=True):\n",
        "        sentences = []\n",
        "        if lang == 'kr':\n",
        "          for s in df:\n",
        "            sentence = unicodedata.normalize('NFKC', s.strip()).translate(self.table)\n",
        "            if generation:\n",
        "              punct = '\"“”#$%&\\'()*+,-/:;<=>@[\\\\]^_`{|}~'\n",
        "              sentence = unicodedata.normalize('NFKC', s.strip()).translate(str.maketrans({key: None for key in punct}))\n",
        "              token_mecab_save = mecab_tokenizer(sentence)\n",
        "              split = sentence.split()\n",
        "              spaced = []\n",
        "              for i,tk in enumerate(token_mecab_save):\n",
        "                token_mecab_save[i] = tk.strip()\n",
        "                \n",
        "              for w in split:\n",
        "                  if w == token_mecab_save[0]:                  \n",
        "                      spaced.append(token_mecab_save[0])\n",
        "                      del token_mecab_save[0]\n",
        "                  else:\n",
        "                      spaced.append(token_mecab_save[0])\n",
        "                      temp = token_mecab_save[0]\n",
        "                      del token_mecab_save[0]\n",
        "                      while token_mecab_save:\n",
        "                          if w != temp:\n",
        "                              spaced.append(\"##\" + token_mecab_save[0]) # 앞에 ##를 붙인다\n",
        "                              temp = temp+token_mecab_save[0]\n",
        "                              del token_mecab_save[0]\n",
        "                          else:\n",
        "                              break\n",
        "            else:\n",
        "              spaced= mecab_tokenizer(sentence)\n",
        "            sentences.append(spaced)\n",
        "        elif lang == 'en':\n",
        "          for s in df:\n",
        "            spaced = mt.tokenize(self.normalizeString(s))\n",
        "            sentences.append(spaced)\n",
        "        # else:\n",
        "        #     print('pick English or Korean')\n",
        "        return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FOS0fVas3L9"
      },
      "source": [
        "import time\n",
        "k='원문'\n",
        "e='번역문'\n",
        "\n",
        "data = df.copy()\n",
        "\n",
        "mecab_tokenizer=Mecab().morphs\n",
        "mt = MosesTokenizer()\n",
        "\n",
        "en_ko_pre = preprocessing()\n",
        "\n",
        "start = time.time()\n",
        "X = en_ko_pre.extract_stem(df[e],lang='en')\n",
        "Y = en_ko_pre.extract_stem(data[k],lang='kr')\n",
        "  \n",
        "with open(f'after_mecab_kr.txt', 'w', encoding='utf-8') as f:\n",
        "    for line in Y:\n",
        "        f.write(' '.join(line)+'\\n')\n",
        "\n",
        "with open(f'after_mecab_en.txt', 'w', encoding='utf-8') as f:\n",
        "    for line in X:\n",
        "        f.write(' '.join(line)+'\\n')\n",
        "f.close()\n",
        "\n",
        "print('Total preprocessing time:', time.time()-start)\n",
        "print(X)\n",
        "print(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5Z_iyjkw8eO"
      },
      "source": [
        "##Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "gYQ_6Xm8w4Ko",
        "outputId": "e37a0aa3-d8a7-4cff-df88-77a6ff7703a0"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_table('/content/drive/MyDrive/OCR/dataset/after_mecab_en.txt', sep='\\n',header=None,names=['input'])\n",
        "Y = pd.read_table('/content/drive/MyDrive/OCR/dataset/after_mecab_kr.txt', sep='\\n',header=None,names=['target'])\n",
        "\n",
        "\n",
        "df['target'] = Y\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gangneung maehwajeon one of the twelve madang ...</td>\n",
              "      <td>강릉 기생 매화 ##가 등장 ##하 ##는 판소리 열 ##두 ##마당 ##의 하나 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>the purpose of the establishment was to hold v...</td>\n",
              "      <td>다양 ##한 미술 ##관련 전시회 ##의 개최 각종 교육 ##프로그램 ##과 새로운...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>there are two main types of soy sauce home bre...</td>\n",
              "      <td>간장 ##은 가정 ##에서 담구 ##던 재래식 간장 ##과 공장 ##에서 양조 ##...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>he occupies a significant position in korean h...</td>\n",
              "      <td>그 ##는 한국 ##에 발 ##을 내딛 ##은 최조 ##의 가톨릭 사제 ##로 한국...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>it is unclear regarding the foundation of deok...</td>\n",
              "      <td>덕사 ##의 창건 ##은 신라 말 고려 초 ##라고 전해 ##지나 문헌 ##이 없 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1302028</th>\n",
              "      <td>the proposed reduction of the service period w...</td>\n",
              "      <td>복무 ##기간 단축 ##안 ##은 10 ##월 전 ##역자 ##부터 2 ##주 단위...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1302029</th>\n",
              "      <td>in fact the vice chief of the seoul eastern di...</td>\n",
              "      <td>실제로 이번 인사 ##에서 인천 ##지검 특수 ##부장 서울 ##중앙 ##지검 증권...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1302030</th>\n",
              "      <td>on the th at a meeting of experts on how to tr...</td>\n",
              "      <td>29 ##일 서울 서초구 한국 ##산업 ##기술 ##진흥 ##협회 중 ##회의실 #...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1302031</th>\n",
              "      <td>the gwangju office of education said there was...</td>\n",
              "      <td>광주시 ##교육청 ##은 지난 1 ##일 ##과 2 ##일 한 ##유총 광주 ##지...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1302032</th>\n",
              "      <td>the appointment for the head of the science an...</td>\n",
              "      <td>과학 ##기술 ##정보 ##통신부 과학 ##기술 ##혁신 ##본부 ##장 ##에 #...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1302033 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     input                                             target\n",
              "0        gangneung maehwajeon one of the twelve madang ...  강릉 기생 매화 ##가 등장 ##하 ##는 판소리 열 ##두 ##마당 ##의 하나 ...\n",
              "1        the purpose of the establishment was to hold v...  다양 ##한 미술 ##관련 전시회 ##의 개최 각종 교육 ##프로그램 ##과 새로운...\n",
              "2        there are two main types of soy sauce home bre...  간장 ##은 가정 ##에서 담구 ##던 재래식 간장 ##과 공장 ##에서 양조 ##...\n",
              "3        he occupies a significant position in korean h...  그 ##는 한국 ##에 발 ##을 내딛 ##은 최조 ##의 가톨릭 사제 ##로 한국...\n",
              "4        it is unclear regarding the foundation of deok...  덕사 ##의 창건 ##은 신라 말 고려 초 ##라고 전해 ##지나 문헌 ##이 없 ...\n",
              "...                                                    ...                                                ...\n",
              "1302028  the proposed reduction of the service period w...  복무 ##기간 단축 ##안 ##은 10 ##월 전 ##역자 ##부터 2 ##주 단위...\n",
              "1302029  in fact the vice chief of the seoul eastern di...  실제로 이번 인사 ##에서 인천 ##지검 특수 ##부장 서울 ##중앙 ##지검 증권...\n",
              "1302030  on the th at a meeting of experts on how to tr...  29 ##일 서울 서초구 한국 ##산업 ##기술 ##진흥 ##협회 중 ##회의실 #...\n",
              "1302031  the gwangju office of education said there was...  광주시 ##교육청 ##은 지난 1 ##일 ##과 2 ##일 한 ##유총 광주 ##지...\n",
              "1302032  the appointment for the head of the science an...  과학 ##기술 ##정보 ##통신부 과학 ##기술 ##혁신 ##본부 ##장 ##에 #...\n",
              "\n",
              "[1302033 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BB0fdIAyInf"
      },
      "source": [
        "tk = pd.concat([df['input'],Y['target']])\n",
        "tk.to_csv('after_mecab.txt', sep = '\\t', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pQO82wDJWFo"
      },
      "source": [
        "!pip install sentencepiece\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfcoWcDdbQaN"
      },
      "source": [
        "%cd /content/drive/MyDrive/OCR/model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9r2eLCmKkH-"
      },
      "source": [
        "import os\n",
        "import sentencepiece as spm\n",
        "\n",
        "vocab_size = 64000\n",
        "\n",
        "model_type = 'unigram'\n",
        "character_coverage  = 1.0  # 전체를 cover 하기 위해, default=0.9995\n",
        "user_defined_symbols = '[SEP],[MASK],[BOS],[EOS],[UNK0],[UNK1],[UNK2],[UNK3],[UNK4],[UNK5],[UNK6],[UNK7],[UNK8],[UNK9],[unused0],[unused1],[unused2],[unused3],[unused4],[unused5],[unused6],[unused7],[unused8],[unused9],[unused10],[unused11],[unused12],[unused13],[unused14],[unused15],[unused16],[unused17],[unused18],[unused19],[unused20],[unused21],[unused22],[unused23],[unused24],[unused25],[unused26],[unused27],[unused28],[unused29],[unused30],[unused31],[unused32],[unused33],[unused34],[unused35],[unused36],[unused37],[unused38],[unused39],[unused40],[unused41],[unused42],[unused43],[unused44],[unused45],[unused46],[unused47],[unused48],[unused49],[unused50],[unused51],[unused52],[unused53],[unused54],[unused55],[unused56],[unused57],[unused58],[unused59],[unused60],[unused61],[unused62],[unused63],[unused64],[unused65],[unused66],[unused67],[unused68],[unused69],[unused70],[unused71],[unused72],[unused73],[unused74],[unused75],[unused76],[unused77],[unused78],[unused79],[unused80],[unused81],[unused82],[unused83],[unused84],[unused85],[unused86],[unused87],[unused88],[unused89],[unused90],[unused91],[unused92],[unused93],[unused94],[unused95],[unused96],[unused97],[unused98],[unused99]'\n",
        "# unused_token_num = 180\n",
        "# unused_list = ['[unused{}]'.format(n) for n in range(unused_token_num)]\n",
        "# user_defined_symbols = user_defined_symbols + unused_list\n",
        "\n",
        "print(user_defined_symbols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJVu0uu3E1pj"
      },
      "source": [
        "max_len=0\n",
        "for i in df['input']:\n",
        "  if len(i) > max_len:\n",
        "    max_len = len(i)\n",
        "\n",
        "for i in df['target']:\n",
        "  if len(i) > max_len:\n",
        "    max_len = len(i) \n",
        "\n",
        "print(max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnJgVkydNaQ1"
      },
      "source": [
        "input_argument = '--input=%s --model_prefix=%s --vocab_size=%s --model_type=%s --character_coverage=%s --max_sentence_length=786 --pad_id=0 --pad_piece=[PAD] --bos_id=1 --bos_piece=[CLS] --eos_id=3  --eos_piece=[SEP] --unk_id=2 --unk_piece=[UNK] --user_defined_symbols=%s'\n",
        "\n",
        "cmd = input_argument%('/content/drive/MyDrive/OCR/dataset/after_mecab.txt', 'spm_tokenizer', vocab_size, model_type, character_coverage, user_defined_symbols)\n",
        "\n",
        "spm.SentencePieceTrainer.Train(cmd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yt5IYhELP2P"
      },
      "source": [
        "print(cmd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpcvYN6Bd1fP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16d589f7-19a4-4ecf-d621-3ce14a630f49"
      },
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load('/content/drive/MyDrive/OCR/model/spm/spm_tokenizer.model')\n",
        "\n",
        "mecab_tokenizer=Mecab().morphs\n",
        "mt = MosesTokenizer()\n",
        "\n",
        "en_ko_pre = preprocessing()\n",
        "\n",
        "#sentence = 'gangneung maehwajeon one of the twelve madang of pansori . '\n",
        "sentence = en_ko_pre.extract_stem([\"I'm in love with you.\"],lang='en')\n",
        "for line in sentence:\n",
        "    sentence = ' '.join(line)+'\\n'\n",
        "\n",
        "print(sentence)\n",
        "tokens = sp.encode_as_pieces(sentence)\n",
        "ids = sp.encode_as_ids(sentence)\n",
        "\n",
        "print(ids)\n",
        "print(type(ids))\n",
        "print(sp.piece_to_id(tokens))\n",
        "print(tokens)\n",
        "\n",
        "# tokens = sp.decode_pieces(tokens)\n",
        "# ids = sp.decode_ids(ids)\n",
        "\n",
        "# print(ids)\n",
        "# print(tokens)\n",
        "\n",
        "max_len = 13\n",
        "n_to_pad = max_len - len(tokens)\n",
        "\n",
        "\n",
        "tokens = ['[CLS]', ] + sp.encode_as_pieces(sentence) + ['[PAD]', ]*n_to_pad + ['[SEP]', ]\n",
        "\n",
        "print(tokens)\n",
        "print(sp.piece_to_id(tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unicode\n",
            "i'm in love with you.\n",
            "normal i m in love with you .\n",
            "i m in love with you .\n",
            "\n",
            "[155, 459, 134, 1134, 164, 183, 119]\n",
            "<class 'list'>\n",
            "[155, 459, 134, 1134, 164, 183, 119]\n",
            "['▁i', '▁m', '▁in', '▁love', '▁with', '▁you', '▁.']\n",
            "['[CLS]', '▁i', '▁m', '▁in', '▁love', '▁with', '▁you', '▁.', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[SEP]']\n",
            "[1, 155, 459, 134, 1134, 164, 183, 119, 0, 0, 0, 0, 0, 0, 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VMUtFTpz2oS"
      },
      "source": [
        "%cd /content/drive/MyDrive/OCR/dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBmhYPmybikl"
      },
      "source": [
        "#CLS=1, SEP=3\n",
        "#데이터 단위가 클 경우 tf.utils.Sequence 사용 - https://jins-sw.tistory.com/6\n",
        "from tqdm import tqdm\n",
        "import sentencepiece as spm\n",
        "import numpy as np\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load('/content/drive/MyDrive/OCR/model/spm_tokenizer.model')\n",
        "\n",
        "def tokenizing(df, max_len=0):\n",
        "    ids=[]\n",
        "    #tokens=[]\n",
        "    # for s in tqdm(df, desc='tokenizing'):\n",
        "    #     token = sp.encode_as_pieces(s)\n",
        "    #     tokens.append(token) #append token\n",
        "\n",
        "    # if max_len == 0: #check max length of token\n",
        "    #     max_len = max(len(token) for token in tokens)\n",
        "    \n",
        "    #i=0\n",
        "    for s in tqdm(df, desc='padding'):      \n",
        "        # if len(tokens) >= max_len: #Truncate if tokens are longer than max_len\n",
        "        #     result.append(tokens[:max_len])\n",
        "        # else:\n",
        "        token = sp.encode_as_pieces(s)\n",
        "        n_to_pad = max_len - len(token) \n",
        "        token = ['[CLS]', ] + token + ['[PAD]', ]*n_to_pad + ['[SEP]', ]\n",
        "        ids.append(sp.piece_to_id(token)) #append id\n",
        "        #i+=1\n",
        "    \n",
        "    ids = np.array(ids)\n",
        "    \n",
        "    return ids\n",
        "\n",
        "id_input = tokenizing(df['input'], max_len=145)\n",
        "id_output = tokenizing(df['target'], max_len=220)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoAnDTMZWuD5"
      },
      "source": [
        "#df[['id_input','id_output','token_input','token_output']].to_pickle('/content/drive/MyDrive/OCR/dataset/tokenized')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8J1C4gX0maq"
      },
      "source": [
        "#df[['id_input','id_output','token_input','token_output']].to_csv('/content/drive/MyDrive/OCR/dataset/tokenized.csv',index=False) #Recommend saving df into pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ws80mrsRGV36"
      },
      "source": [
        "import pickle\n",
        "\n",
        "filename ='/content/drive/MyDrive/OCR/dataset/id_input.pkl'\n",
        "pickle.dump(id_input, open(filename, 'wb'))\n",
        "\n",
        "filename = '/content/drive/MyDrive/OCR/dataset/id_output.pkl'\n",
        "pickle.dump(id_output, open(filename, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_t_ucRKjRmG"
      },
      "source": [
        "def read_sentencepiece_vocab(filepath):\n",
        "  voc = []\n",
        "  with open(filepath, encoding='utf-8') as fi:\n",
        "    for line in fi:\n",
        "      voc.append(line.split(\"\\t\")[0])\n",
        "  # skip the first <unk> token\n",
        "  voc = voc[1:]\n",
        "  return voc\n",
        "\n",
        "snt_vocab = read_sentencepiece_vocab(\"{}.vocab\".format(MODEL_PREFIX))\n",
        "print(\"Learnt vocab size: {}\".format(len(snt_vocab)))\n",
        "print(\"Sample tokens: {}\".format(random.sample(snt_vocab, 10)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJ3Vq4zt8z4P"
      },
      "source": [
        "##Make dataset for fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Va2h0HDU8y6F"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhdAbBj4gsvh",
        "outputId": "e0750132-4611-48e5-afdb-adc4b3d5bc91"
      },
      "source": [
        "%cd /content/drive/MyDrive/OCR"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/OCR\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywGbLhn-9H-e"
      },
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# #df = pd.read_csv('/content/drive/MyDrive/OCR/dataset/df_pp.csv', converters={'id_input': eval,'token_input': eval,'token_output': eval,'id_output': eval})\n",
        "# df = pd.read_pickle('/content/drive/MyDrive/OCR/dataset/tokenized')\n",
        "\n",
        "# df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3KyiAOnH5Oi"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('dataset/id_input.pkl', 'rb') as f:\n",
        "    X = pickle.load(f)\n",
        "\n",
        "with open('dataset/id_output.pkl', 'rb') as f:\n",
        "    y = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgNniFhiGZby",
        "outputId": "d7c6b087-0586-43be-b726-3dc155893271"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.008, random_state=10, shuffle=True)\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1291616, 147), (10417, 147), (1291616, 222), (10417, 222))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVvXpOkFPByo"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "\n",
        "X_train_ts = torch.from_numpy(X_train)\n",
        "X_test_ts = torch.from_numpy(X_test)\n",
        "y_train_ts = torch.from_numpy(y_train)\n",
        "y_test_ts = torch.from_numpy(y_test)\n",
        "\n",
        "train_tensor = TensorDataset(X_train_ts, y_train_ts)\n",
        "test_tensor = TensorDataset(X_test_ts, y_test_ts)\n",
        "\n",
        "train_loader = DataLoader(train_tensor, batch_size=128, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0-WK7VJS0QT"
      },
      "source": [
        "from transformers import BertGenerationEncoder, BertGenerationDecoder, EncoderDecoderModel, BertGenerationConfig\n",
        "\n",
        "tokenizer = Berttoken\n",
        "encoder = BertGenerationEncoder.from_pretrained(\"monologg/kobert\")\n",
        "decoder = BertGenerationDecoder.from_pretrained(\"bert-large-uncased\", add_cross_attention=True, is_decoder=True)\n",
        "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)\n",
        "#optim = AdamW(model.parameters(), lr=5e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afkzc2STTmcG"
      },
      "source": [
        "!pip install pytorch_pretrained_bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCBVJabPTSeA"
      },
      "source": [
        "from pytorch_pretrained_bert import BertAdam\n",
        "# Prepare optimizer\n",
        "param_optimizer = list(model.named_parameters())\n",
        "\n",
        "# hack to remove pooler, which is not used\n",
        "# thus it produce None grad that break apex\n",
        "param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "optim = BertAdam(optimizer_grouped_parameters,\n",
        "                             lr=5e-5,\n",
        "                             warmup=0.1,\n",
        "                             t_total=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FQd46OPSkt0"
      },
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "model.to('cuda')\n",
        "model.train()\n",
        "\n",
        "losses = []\n",
        "for epoch in range(50):\n",
        "    start = time.time()\n",
        "    for batch_X, batch_y in tqdm(train_loader, desc=f'Epoch{epoch}'):\n",
        "        for seq_X, seq_y in zip(batch_X, batch_y):\n",
        "            optim.zero_grad()\n",
        "            seq_X = batch_X.to('cuda')\n",
        "            seq_y = batch_y.to('cuda')\n",
        "            loss = model(input_ids=batch_X, decoder_input_ids=batch_y, labels=batch_y).loss\n",
        "            eveloss += loss.mean().item()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "    losses.append(eve_loss)\n",
        "    print(\"Epoch\",epoch,\">> LOSS : \",eveloss,\" TRAINING TIME : \", time.time()-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76fS11qpSl9M"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWxRD-ZQvKry"
      },
      "source": [
        "##Another tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl3sJho53TyK"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "2XFgZ-pEerDx",
        "outputId": "41b780e5-476e-409d-f41a-d5c3c8cc0eb4"
      },
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import MBartForConditionalGeneration, MBartTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "import torch\n",
        "from torch.utils.data import random_split"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-3c4737397f0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMBartForConditionalGeneration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMBartTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeq2SeqTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeq2SeqTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Seq2SeqTrainingArguments' from 'transformers' (/usr/local/lib/python3.7/dist-packages/transformers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymj_gM3moZO6",
        "outputId": "41749679-3e13-412d-efca-ce074452a9ed"
      },
      "source": [
        "data = []\n",
        "for src, tgt in zip(df['input'],df['target']):\n",
        "      data.append(\n",
        "          {\n",
        "              \"translation\": {\n",
        "                  \"en\": src.strip(),\n",
        "                  \"ko\": tgt.strip()\n",
        "              }\n",
        "          }\n",
        "      )\n",
        "print(f'total size of data is {len(data)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total size of data is 1302033\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArF7QUId10nb"
      },
      "source": [
        "max_input_length = 147\n",
        "max_target_length = 222\n",
        "source_lang = \"en_XX\"\n",
        "target_lang = \"ko\"\n",
        "\n",
        "def data_collator(features:list):\n",
        "  labels = [f[\"translation\"][\"ko\"] for f in features]\n",
        "  inputs = [f[\"translation\"][\"en\"] for f in features]\n",
        "\n",
        "  batch = tokenizer.prepare_seq2seq_batch(src_texts=inputs, src_lang=source_lang, tgt_lang=target_lang, tgt_texts=labels, max_length=max_input_length, max_target_length=max_target_length)\n",
        "\n",
        "  for k in batch:\n",
        "    batch[k] = torch.tensor(batch[k])\n",
        "\n",
        "  return batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "EPFrLCAmmMlg",
        "outputId": "b7d27438-19e1-421d-ffe4-d54625fedc32"
      },
      "source": [
        "# initiating model, tokenizer\n",
        "model = MBartForConditionalGeneration.from_pretrained(\"hyunwoongko/kobart\")\n",
        "tokenizer = MBartTokenizer.from_pretrained(\"hyunwoongko/kobart\")\n",
        "\n",
        "# defining training related arguments\n",
        "args = Seq2SeqTrainingArguments(output_dir=\"/content/drive/MyDrive/OCR/model/translator\",\n",
        "                        do_train=True,\n",
        "                        do_eval=True,\n",
        "                        evaluation_strategy=\"epoch\",\n",
        "                        per_device_train_batch_size=64,\n",
        "                        per_device_eval_batch_size=64,\n",
        "                        learning_rate=5e-5,\n",
        "                        num_train_epochs=2,\n",
        "                        logging_dir=\"/logs\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-6a0a6a1bcb46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# initiating model, tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMBartForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hyunwoongko/kobart\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMBartTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hyunwoongko/kobart\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# defining training related arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MBartForConditionalGeneration' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRRKyRJM2_nV"
      },
      "source": [
        "trainer = Seq2SeqTrainer(model=model, \n",
        "                args=args, \n",
        "                data_collator=data_collator, \n",
        "                train_dataset=[X_train, y_train], \n",
        "                eval_dataset=eval_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-2sLyt36m1e"
      },
      "source": [
        "##Git 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wWnhebP6gW-",
        "outputId": "468e5bbd-b04f-404c-ed46-88768b4308b2"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.008, random_state=10, shuffle=True)\n",
        "train.shape, test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1291616, 2), (10417, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRTOjbF38oKf",
        "outputId": "fadef038-8a21-4151-e2b7-e57a914907ca"
      },
      "source": [
        "%cd /content/drive/MyDrive/OCR"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/OCR\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSQkJ9ff7vmS"
      },
      "source": [
        " !pip install git+https://github.com/SKT-AI/KoBART#egg=kobart"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNqr4iSL73q9",
        "outputId": "45bcece9-85bb-452d-cb35-797f7bb67cea"
      },
      "source": [
        "!git clone https://github.com/seujung/KoBART-translation.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'KoBART-translation'...\n",
            "remote: Enumerating objects: 46, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 46 (delta 18), reused 32 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (46/46), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQAlp5sG9n3M",
        "outputId": "c68fe074-91ad-4016-8dd0-b14b98b32a64"
      },
      "source": [
        "%cd KoBART-translation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/OCR/KoBART-translation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3ouv8KF7l4A"
      },
      "source": [
        "train.to_csv('data/train.tsv', sep='\\t')\n",
        "test.to_csv('data/test.tsv', sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFl3uaR68Fdu"
      },
      "source": [
        "!./prepare.sh\n",
        "!pip install -r requirements.txt\n",
        "!python train.py  --gradient_clip_val 1.0 --max_epochs 50 --default_root_dir logs  --gpus 1 --batch_size 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2o6JT04_AYhD"
      },
      "source": [
        "##Following transformer paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2GJKrQXAdhN"
      },
      "source": [
        "!pip install torchtext==0.6.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbeu0npFAnOH"
      },
      "source": [
        "import spacy\n",
        "\n",
        "spacy_en = spacy.load('en') # 영어 토큰화(tokenization)\n",
        "\n",
        "tokenized = spacy_en.tokenizer(\"I am a graduate student.\")\n",
        "\n",
        "for i, token in enumerate(tokenized):\n",
        "    print(f\"인덱스 {i}: {token.text}\")\n",
        "\n",
        "def tokenize_de(text):\n",
        "    return [token.text for token in spacy_de.tokenizer(text)]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}